{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP Synthetic Data vs Historical Exchange Data Comparison\n",
    "\n",
    "This notebook compares statistical properties of:\n",
    "1. **Historical Data**: Hyperliquid L2 book quotes from S3 (24-hour period)\n",
    "2. **Synthetic Data**: Aleatoric MCP-generated market data\n",
    "\n",
    "## Goal\n",
    "Demonstrate that Aleatoric's synthetic data produces similar statistical properties to real exchange data,\n",
    "validating its use for backtesting and strategy development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "# !pip install httpx pandas numpy plotly python-dotenv scipy boto3 pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import ast\n",
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from io import BytesIO\n",
    "\n",
    "import boto3\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis, skew, ks_2samp, mannwhitneyu\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MCP Configuration\n",
    "load_dotenv()\n",
    "MCP_BASE_URL = 'https://mcp.aleatoric.systems'\n",
    "API_KEY = os.getenv('ALEATORIC_API_KEY')\n",
    "CACHE_KEY = os.getenv('ALEATORIC_CACHE_KEY')\n",
    "if not API_KEY:\n",
    "    raise RuntimeError('Set ALEATORIC_API_KEY before calling MCP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Historical Hyperliquid L2 Book Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperliquid_l2book_from_s3(\n",
    "    bucket: str,\n",
    "    prefix: str,\n",
    "    symbol: str,\n",
    "    date: str,\n",
    "    resample_seconds: float = 5.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Hyperliquid L2 book data from S3 and parse into structured format.\n",
    "    \n",
    "    Args:\n",
    "        bucket: S3 bucket name\n",
    "        prefix: S3 key prefix\n",
    "        symbol: Trading symbol (e.g., 'BTC')\n",
    "        date: Date string in YYYYMMDD format\n",
    "        resample_seconds: Resample interval for time-based analysis\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with parsed L2 book data\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Construct S3 key\n",
    "    key = f\"{prefix}/{symbol}/l2Book/{symbol}_l2Book_{date}.parquet\"\n",
    "    print(f\"Loading: s3://{bucket}/{key}\")\n",
    "    \n",
    "    # Download parquet file\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    parquet_data = BytesIO(response['Body'].read())\n",
    "    \n",
    "    # Read parquet\n",
    "    df_raw = pd.read_parquet(parquet_data)\n",
    "    print(f\"Loaded {len(df_raw)} raw records\")\n",
    "    \n",
    "    # Parse the raw field\n",
    "    parsed_records = []\n",
    "    parse_errors = 0\n",
    "    \n",
    "    for idx, row in df_raw.iterrows():\n",
    "        try:\n",
    "            # The raw field is already a dict (not a string)\n",
    "            raw_data = row['raw']\n",
    "            \n",
    "            # Handle case where it might be a string\n",
    "            if isinstance(raw_data, str):\n",
    "                raw_data = ast.literal_eval(raw_data.replace('\\n', ' '))\n",
    "            \n",
    "            data = raw_data.get('data', {})\n",
    "            levels = data.get('levels', [])\n",
    "            \n",
    "            # Handle numpy arrays - convert to list\n",
    "            if hasattr(levels, 'tolist'):\n",
    "                levels = levels.tolist()\n",
    "            \n",
    "            # levels[0] = bids, levels[1] = asks\n",
    "            # Each may also be a numpy array\n",
    "            if len(levels) < 2:\n",
    "                continue\n",
    "                \n",
    "            bids = levels[0]\n",
    "            asks = levels[1]\n",
    "            \n",
    "            # Convert numpy arrays to lists if needed\n",
    "            if hasattr(bids, 'tolist'):\n",
    "                bids = bids.tolist()\n",
    "            if hasattr(asks, 'tolist'):\n",
    "                asks = asks.tolist()\n",
    "            \n",
    "            if not bids or not asks:\n",
    "                continue\n",
    "            \n",
    "            # Best bid/ask\n",
    "            best_bid_px = float(bids[0]['px'])\n",
    "            best_bid_sz = float(bids[0]['sz'])\n",
    "            best_ask_px = float(asks[0]['px'])\n",
    "            best_ask_sz = float(asks[0]['sz'])\n",
    "            \n",
    "            # Mid price and spread\n",
    "            mid_price = (best_bid_px + best_ask_px) / 2\n",
    "            spread = best_ask_px - best_bid_px\n",
    "            spread_bps = (spread / mid_price) * 10000\n",
    "            \n",
    "            # Depth analysis (sum of top N levels)\n",
    "            bid_depth_5 = sum(float(b['sz']) for b in bids[:5])\n",
    "            ask_depth_5 = sum(float(a['sz']) for a in asks[:5])\n",
    "            \n",
    "            bid_depth_10 = sum(float(b['sz']) for b in bids[:10])\n",
    "            ask_depth_10 = sum(float(a['sz']) for a in asks[:10])\n",
    "            \n",
    "            # Imbalance\n",
    "            total_depth = bid_depth_5 + ask_depth_5\n",
    "            imbalance = (bid_depth_5 - ask_depth_5) / total_depth if total_depth > 0 else 0\n",
    "            \n",
    "            # Number of levels\n",
    "            n_bid_levels = len(bids)\n",
    "            n_ask_levels = len(asks)\n",
    "            \n",
    "            parsed_records.append({\n",
    "                'timestamp': pd.to_datetime(row['timestamp']),\n",
    "                'exchange_time': data.get('time', 0),\n",
    "                'mid_price': mid_price,\n",
    "                'best_bid': best_bid_px,\n",
    "                'best_ask': best_ask_px,\n",
    "                'best_bid_size': best_bid_sz,\n",
    "                'best_ask_size': best_ask_sz,\n",
    "                'spread': spread,\n",
    "                'spread_bps': spread_bps,\n",
    "                'bid_depth_5': bid_depth_5,\n",
    "                'ask_depth_5': ask_depth_5,\n",
    "                'bid_depth_10': bid_depth_10,\n",
    "                'ask_depth_10': ask_depth_10,\n",
    "                'imbalance': imbalance,\n",
    "                'n_bid_levels': n_bid_levels,\n",
    "                'n_ask_levels': n_ask_levels,\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            parse_errors += 1\n",
    "            if parse_errors <= 3:\n",
    "                print(f\"   Parse error {parse_errors}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully parsed {len(parsed_records)} records ({parse_errors} errors)\")\n",
    "    \n",
    "    if len(parsed_records) == 0:\n",
    "        raise ValueError(\"No records were successfully parsed!\")\n",
    "    \n",
    "    df = pd.DataFrame(parsed_records)\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['returns'] = df['mid_price'].pct_change()\n",
    "    df['log_returns'] = np.log(df['mid_price'] / df['mid_price'].shift(1))\n",
    "    \n",
    "    # Resample to specified interval for cleaner analysis\n",
    "    if resample_seconds > 0:\n",
    "        df_resampled = df.resample(f'{int(resample_seconds)}s').agg({\n",
    "            'mid_price': 'last',\n",
    "            'best_bid': 'last',\n",
    "            'best_ask': 'last',\n",
    "            'best_bid_size': 'mean',\n",
    "            'best_ask_size': 'mean',\n",
    "            'spread': 'mean',\n",
    "            'spread_bps': 'mean',\n",
    "            'bid_depth_5': 'mean',\n",
    "            'ask_depth_5': 'mean',\n",
    "            'bid_depth_10': 'mean',\n",
    "            'ask_depth_10': 'mean',\n",
    "            'imbalance': 'mean',\n",
    "            'n_bid_levels': 'mean',\n",
    "            'n_ask_levels': 'mean',\n",
    "        }).dropna()\n",
    "        \n",
    "        # Recalculate returns on resampled data\n",
    "        df_resampled['returns'] = df_resampled['mid_price'].pct_change()\n",
    "        df_resampled['log_returns'] = np.log(df_resampled['mid_price'] / df_resampled['mid_price'].shift(1))\n",
    "        \n",
    "        print(f\"Resampled to {len(df_resampled)} records at {resample_seconds}s intervals\")\n",
    "        return df_resampled\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data\n",
    "print(\"Loading historical Hyperliquid L2 book data...\")\n",
    "historical_df = load_hyperliquid_l2book_from_s3(\n",
    "    bucket=S3_CONFIG['bucket'],\n",
    "    prefix=S3_CONFIG['prefix'],\n",
    "    symbol=SYMBOL,\n",
    "    date=DATA_DATE,\n",
    "    resample_seconds=S3_CONFIG['resample_seconds']\n",
    ")\n",
    "\n",
    "print(f\"\\nHistorical Data Shape: {historical_df.shape}\")\n",
    "print(f\"Time Range: {historical_df.index.min()} to {historical_df.index.max()}\")\n",
    "print(f\"Duration: {historical_df.index.max() - historical_df.index.min()}\")\n",
    "historical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical data summary statistics\n",
    "print(\"Historical Data Summary:\")\n",
    "print(\"=\"*60)\n",
    "historical_df.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fetch MCP Synthetic Data with Matching Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Synthetic Data (fetched via export_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Request/validate config for MCP synthetic dataset\n",
    "mcp_config = {\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"seed\": 42,\n",
    "    \"duration_seconds\": int(24 * 3600),\n",
    "    \"tick_size\": 0.01,\n",
    "    \"lot_size\": 0.001,\n",
    "}\n",
    "\n",
    "with httpx.Client(timeout=30) as client:\n",
    "    validation = client.post(\n",
    "        f\"{MCP_BASE_URL}/mcp/config/validate\",\n",
    "        headers={\"X-API-Key\": API_KEY},\n",
    "        json={\"config\": mcp_config},\n",
    "    )\n",
    "    validation.raise_for_status()\n",
    "    validation = validation.json()\n",
    "    cache_key = CACHE_KEY or validation.get(\"cache_key\") or validation.get(\"hash\")\n",
    "    if not cache_key:\n",
    "        raise RuntimeError(\"No cache_key returned. Set ALEATORIC_CACHE_KEY or use a generated dataset.\")\n",
    "    print(f\"Using cache_key: {cache_key}\")\n",
    "    export = client.get(f\"{MCP_BASE_URL}/mcp/caches/export/{cache_key}\", headers={\"X-API-Key\": API_KEY})\n",
    "    export.raise_for_status()\n",
    "    synthetic_df = pd.read_parquet(BytesIO(export.content))\n",
    "    print(f\"Synthetic dataset rows: {len(synthetic_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters from historical data to calibrate synthetic generation\n",
    "historical_returns = historical_df['log_returns'].dropna()\n",
    "\n",
    "# Annualized volatility from historical data\n",
    "dt_seconds = S3_CONFIG['resample_seconds']\n",
    "periods_per_year = (365 * 24 * 3600) / dt_seconds\n",
    "historical_vol_annual = historical_returns.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "# Other parameters\n",
    "initial_price = historical_df['mid_price'].iloc[0]\n",
    "base_spread_bps = historical_df['spread_bps'].median()\n",
    "num_steps = len(historical_df)\n",
    "\n",
    "print(\"Calibrated Parameters from Historical Data:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Initial Price: ${initial_price:,.2f}\")\n",
    "print(f\"Annualized Volatility: {historical_vol_annual*100:.2f}%\")\n",
    "print(f\"Median Spread: {base_spread_bps:.2f} bps\")\n",
    "print(f\"Number of Steps: {num_steps}\")\n",
    "print(f\"Time Step: {dt_seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data summary\n",
    "print(\"Synthetic Data Summary:\")\n",
    "print(\"=\"*60)\n",
    "synthetic_df.describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Properties Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(df: pd.DataFrame, name: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute comprehensive statistics for a market data DataFrame.\"\"\"\n",
    "    returns = df['log_returns'].dropna()\n",
    "    spreads = df['spread_bps'].dropna()\n",
    "    \n",
    "    # Returns statistics\n",
    "    stats_dict = {\n",
    "        'name': name,\n",
    "        # Returns distribution\n",
    "        'returns_mean_bps': returns.mean() * 10000,\n",
    "        'returns_std_bps': returns.std() * 10000,\n",
    "        'returns_skewness': skew(returns),\n",
    "        'returns_kurtosis': kurtosis(returns),\n",
    "        'returns_min_bps': returns.min() * 10000,\n",
    "        'returns_max_bps': returns.max() * 10000,\n",
    "        'returns_median_bps': returns.median() * 10000,\n",
    "        \n",
    "        # Volatility (annualized)\n",
    "        'volatility_annual': returns.std() * np.sqrt(periods_per_year) * 100,\n",
    "        \n",
    "        # Spread statistics\n",
    "        'spread_mean_bps': spreads.mean(),\n",
    "        'spread_std_bps': spreads.std(),\n",
    "        'spread_median_bps': spreads.median(),\n",
    "        'spread_min_bps': spreads.min(),\n",
    "        'spread_max_bps': spreads.max(),\n",
    "        \n",
    "        # Depth statistics\n",
    "        'bid_depth_5_mean': df['bid_depth_5'].mean(),\n",
    "        'ask_depth_5_mean': df['ask_depth_5'].mean(),\n",
    "        \n",
    "        # Imbalance statistics\n",
    "        'imbalance_mean': df['imbalance'].mean(),\n",
    "        'imbalance_std': df['imbalance'].std(),\n",
    "        \n",
    "        # Price statistics\n",
    "        'price_mean': df['mid_price'].mean(),\n",
    "        'price_std': df['mid_price'].std(),\n",
    "        'price_range_pct': (df['mid_price'].max() - df['mid_price'].min()) / df['mid_price'].mean() * 100,\n",
    "    }\n",
    "    \n",
    "    # Autocorrelation of returns (should be ~0 for efficient markets)\n",
    "    if len(returns) > 10:\n",
    "        stats_dict['returns_acf_1'] = returns.autocorr(lag=1) if hasattr(returns, 'autocorr') else np.nan\n",
    "        \n",
    "    # Autocorrelation of squared returns (volatility clustering)\n",
    "    sq_returns = returns ** 2\n",
    "    if len(sq_returns) > 10:\n",
    "        stats_dict['sq_returns_acf_1'] = sq_returns.autocorr(lag=1) if hasattr(sq_returns, 'autocorr') else np.nan\n",
    "    \n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for both datasets\n",
    "hist_stats = compute_statistics(historical_df, \"Historical (Hyperliquid)\")\n",
    "synth_stats = compute_statistics(synthetic_df, \"Synthetic (MCP)\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_stats = pd.DataFrame([hist_stats, synth_stats]).set_index('name').T\n",
    "\n",
    "# Calculate percentage difference\n",
    "comparison_stats['Diff %'] = (\n",
    "    (comparison_stats['Synthetic (MCP)'] - comparison_stats['Historical (Hyperliquid)']) \n",
    "    / comparison_stats['Historical (Hyperliquid)'].abs() * 100\n",
    ").round(2)\n",
    "\n",
    "print(\"Statistical Properties Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_stats.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for distribution similarity\n",
    "hist_returns = historical_df['log_returns'].dropna()\n",
    "synth_returns = synthetic_df['log_returns'].dropna()\n",
    "\n",
    "hist_spreads = historical_df['spread_bps'].dropna()\n",
    "synth_spreads = synthetic_df['spread_bps'].dropna()\n",
    "\n",
    "# Kolmogorov-Smirnov test (distribution similarity)\n",
    "ks_returns = ks_2samp(hist_returns, synth_returns)\n",
    "ks_spreads = ks_2samp(hist_spreads, synth_spreads)\n",
    "\n",
    "# Mann-Whitney U test (median comparison)\n",
    "mw_returns = mannwhitneyu(hist_returns, synth_returns, alternative='two-sided')\n",
    "mw_spreads = mannwhitneyu(hist_spreads, synth_spreads, alternative='two-sided')\n",
    "\n",
    "print(\"Distribution Similarity Tests:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKolmogorov-Smirnov Test (H0: same distribution):\")\n",
    "print(f\"  Returns: KS={ks_returns.statistic:.4f}, p-value={ks_returns.pvalue:.4e}\")\n",
    "print(f\"  Spreads: KS={ks_spreads.statistic:.4f}, p-value={ks_spreads.pvalue:.4e}\")\n",
    "\n",
    "print(\"\\nMann-Whitney U Test (H0: same median):\")\n",
    "print(f\"  Returns: U={mw_returns.statistic:.0f}, p-value={mw_returns.pvalue:.4e}\")\n",
    "print(f\"  Spreads: U={mw_spreads.statistic:.0f}, p-value={mw_spreads.pvalue:.4e}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  Returns distribution match: {'Good' if ks_returns.pvalue > 0.01 else 'Differs'} (p={ks_returns.pvalue:.4f})\")\n",
    "print(f\"  Spreads distribution match: {'Good' if ks_spreads.pvalue > 0.01 else 'Differs'} (p={ks_spreads.pvalue:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price and Spread Time Series Comparison\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Historical: Mid Price\",\n",
    "        \"Synthetic: Mid Price\",\n",
    "        \"Historical: Spread (bps)\",\n",
    "        \"Synthetic: Spread (bps)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Historical price\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=historical_df.index, y=historical_df['mid_price'], \n",
    "               name=\"Historical Price\", line=dict(color=\"blue\", width=1)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Synthetic price\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=synthetic_df.index, y=synthetic_df['mid_price'], \n",
    "               name=\"Synthetic Price\", line=dict(color=\"orange\", width=1)),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Historical spread\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=historical_df.index, y=historical_df['spread_bps'], \n",
    "               name=\"Historical Spread\", line=dict(color=\"green\", width=1)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Synthetic spread\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=synthetic_df.index, y=synthetic_df['spread_bps'], \n",
    "               name=\"Synthetic Spread\", line=dict(color=\"purple\", width=1)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Time Series Comparison: Historical vs Synthetic\", showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Distribution Comparison\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"Returns Distribution\",\n",
    "        \"Q-Q Plot (Returns)\",\n",
    "        \"Spread Distribution\",\n",
    "        \"Depth Distribution\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Returns histograms\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=hist_returns, name=\"Historical\", opacity=0.7, \n",
    "                 histnorm=\"probability density\", marker_color=\"blue\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=synth_returns, name=\"Synthetic\", opacity=0.7, \n",
    "                 histnorm=\"probability density\", marker_color=\"orange\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Q-Q Plot for returns\n",
    "# Sort returns for Q-Q plot\n",
    "hist_sorted = np.sort(hist_returns)\n",
    "synth_sorted = np.sort(synth_returns)\n",
    "\n",
    "# Resample to same length for comparison\n",
    "n_points = min(len(hist_sorted), len(synth_sorted), 1000)\n",
    "hist_quantiles = np.percentile(hist_returns, np.linspace(0, 100, n_points))\n",
    "synth_quantiles = np.percentile(synth_returns, np.linspace(0, 100, n_points))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hist_quantiles, y=synth_quantiles, mode='markers',\n",
    "               name=\"Q-Q\", marker=dict(color=\"green\", size=3)),\n",
    "    row=1, col=2\n",
    ")\n",
    "# Add 45-degree line\n",
    "min_val = min(hist_quantiles.min(), synth_quantiles.min())\n",
    "max_val = max(hist_quantiles.max(), synth_quantiles.max())\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines',\n",
    "               name=\"y=x\", line=dict(color=\"red\", dash=\"dash\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Spread histograms\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=hist_spreads, name=\"Hist Spread\", opacity=0.7,\n",
    "                 histnorm=\"probability density\", marker_color=\"blue\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=synth_spreads, name=\"Synth Spread\", opacity=0.7,\n",
    "                 histnorm=\"probability density\", marker_color=\"orange\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Depth histograms\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=historical_df['bid_depth_5'], name=\"Hist Depth\", opacity=0.7,\n",
    "                 histnorm=\"probability density\", marker_color=\"blue\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=synthetic_df['bid_depth_5'], name=\"Synth Depth\", opacity=0.7,\n",
    "                 histnorm=\"probability density\", marker_color=\"orange\"),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Distribution Comparison\", barmode='overlay')\n",
    "fig.update_xaxes(title_text=\"Historical Quantiles\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Synthetic Quantiles\", row=1, col=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation Comparison (Volatility Clustering)\n",
    "def compute_acf(series: pd.Series, nlags: int = 30) -> np.ndarray:\n",
    "    \"\"\"Compute autocorrelation function.\"\"\"\n",
    "    acf = np.zeros(nlags)\n",
    "    series = series.dropna()\n",
    "    mean = series.mean()\n",
    "    var = series.var()\n",
    "    if var == 0:\n",
    "        return acf\n",
    "    for lag in range(nlags):\n",
    "        if lag == 0:\n",
    "            acf[lag] = 1.0\n",
    "        else:\n",
    "            acf[lag] = ((series.iloc[lag:].values - mean) * (series.iloc[:-lag].values - mean)).mean() / var\n",
    "    return acf\n",
    "\n",
    "# ACF of squared returns (volatility clustering indicator)\n",
    "hist_sq_returns_acf = compute_acf(hist_returns ** 2, nlags=30)\n",
    "synth_sq_returns_acf = compute_acf(synth_returns ** 2, nlags=30)\n",
    "\n",
    "# ACF of spreads\n",
    "hist_spread_acf = compute_acf(hist_spreads, nlags=30)\n",
    "synth_spread_acf = compute_acf(synth_spreads, nlags=30)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\n",
    "        \"ACF of Squared Returns (Volatility Clustering)\",\n",
    "        \"ACF of Spreads (Persistence)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "lags = list(range(30))\n",
    "\n",
    "# Squared returns ACF\n",
    "fig.add_trace(\n",
    "    go.Bar(x=lags, y=hist_sq_returns_acf, name=\"Historical\", marker_color=\"blue\", opacity=0.7),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=lags, y=synth_sq_returns_acf, name=\"Synthetic\", marker_color=\"orange\", opacity=0.7),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Spread ACF\n",
    "fig.add_trace(\n",
    "    go.Bar(x=lags, y=hist_spread_acf, name=\"Hist Spread\", marker_color=\"blue\", opacity=0.7),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=lags, y=synth_spread_acf, name=\"Synth Spread\", marker_color=\"orange\", opacity=0.7),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add confidence bounds\n",
    "conf_bound = 1.96 / np.sqrt(len(hist_returns))\n",
    "for col in [1, 2]:\n",
    "    fig.add_hline(y=conf_bound, line_dash=\"dash\", line_color=\"red\", row=1, col=col)\n",
    "    fig.add_hline(y=-conf_bound, line_dash=\"dash\", line_color=\"red\", row=1, col=col)\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Autocorrelation Comparison\", barmode='group')\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nVolatility Clustering (ACF of Squared Returns):\")\n",
    "print(f\"  Historical ACF(1): {hist_sq_returns_acf[1]:.4f}\")\n",
    "print(f\"  Synthetic ACF(1):  {synth_sq_returns_acf[1]:.4f}\")\n",
    "print(f\"  Historical ACF(5): {hist_sq_returns_acf[5]:.4f}\")\n",
    "print(f\"  Synthetic ACF(5):  {synth_sq_returns_acf[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intraday Patterns Comparison\n",
    "# Add hour column for both datasets\n",
    "historical_df_hourly = historical_df.copy()\n",
    "historical_df_hourly['hour'] = historical_df_hourly.index.hour\n",
    "\n",
    "synthetic_df_hourly = synthetic_df.copy()\n",
    "synthetic_df_hourly['hour'] = synthetic_df_hourly.index.hour\n",
    "\n",
    "# Compute hourly averages\n",
    "hist_hourly = historical_df_hourly.groupby('hour').agg({\n",
    "    'spread_bps': 'mean',\n",
    "    'bid_depth_5': 'mean',\n",
    "    'log_returns': lambda x: x.std() * np.sqrt(periods_per_year) * 100  # Annualized vol\n",
    "}).rename(columns={'log_returns': 'volatility'})\n",
    "\n",
    "synth_hourly = synthetic_df_hourly.groupby('hour').agg({\n",
    "    'spread_bps': 'mean',\n",
    "    'bid_depth_5': 'mean',\n",
    "    'log_returns': lambda x: x.std() * np.sqrt(periods_per_year) * 100\n",
    "}).rename(columns={'log_returns': 'volatility'})\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"Hourly Spread\", \"Hourly Depth\", \"Hourly Volatility\")\n",
    ")\n",
    "\n",
    "hours = list(range(24))\n",
    "\n",
    "# Spread by hour\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=hist_hourly['spread_bps'], name=\"Historical\", \n",
    "               mode='lines+markers', line=dict(color=\"blue\")),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=synth_hourly['spread_bps'], name=\"Synthetic\", \n",
    "               mode='lines+markers', line=dict(color=\"orange\")),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Depth by hour\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=hist_hourly['bid_depth_5'], name=\"Hist Depth\", \n",
    "               mode='lines+markers', line=dict(color=\"blue\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=synth_hourly['bid_depth_5'], name=\"Synth Depth\", \n",
    "               mode='lines+markers', line=dict(color=\"orange\")),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Volatility by hour\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=hist_hourly['volatility'], name=\"Hist Vol\", \n",
    "               mode='lines+markers', line=dict(color=\"blue\")),\n",
    "    row=1, col=3\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=hours, y=synth_hourly['volatility'], name=\"Synth Vol\", \n",
    "               mode='lines+markers', line=dict(color=\"orange\")),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Intraday Patterns Comparison\")\n",
    "fig.update_xaxes(title_text=\"Hour (UTC)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Hour (UTC)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Hour (UTC)\", row=1, col=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dashboard\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=3,\n",
    "    specs=[\n",
    "        [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
    "        [{\"colspan\": 3}, None, None],\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]\n",
    "    ],\n",
    "    subplot_titles=(\n",
    "        \"Volatility Match\", \"Spread Match\", \"Kurtosis Match\",\n",
    "        \"Price Overlay Comparison\",\n",
    "        \"Returns Stats\", \"Spread Stats\", \"Depth Stats\"\n",
    "    ),\n",
    "    row_heights=[0.2, 0.4, 0.4]\n",
    ")\n",
    "\n",
    "# Indicators\n",
    "vol_match_pct = 100 - abs(comparison_stats.loc['volatility_annual', 'Diff %'])\n",
    "spread_match_pct = 100 - abs(comparison_stats.loc['spread_mean_bps', 'Diff %'])\n",
    "kurt_match_pct = 100 - min(abs(comparison_stats.loc['returns_kurtosis', 'Diff %']), 100)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=vol_match_pct,\n",
    "        title={'text': \"Volatility\"},\n",
    "        gauge={'axis': {'range': [0, 100]}, 'bar': {'color': \"green\" if vol_match_pct > 80 else \"orange\"}}\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=spread_match_pct,\n",
    "        title={'text': \"Spread\"},\n",
    "        gauge={'axis': {'range': [0, 100]}, 'bar': {'color': \"green\" if spread_match_pct > 80 else \"orange\"}}\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=kurt_match_pct,\n",
    "        title={'text': \"Kurtosis\"},\n",
    "        gauge={'axis': {'range': [0, 100]}, 'bar': {'color': \"green\" if kurt_match_pct > 50 else \"orange\"}}\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Price overlay\n",
    "# Normalize prices to start at 100 for comparison\n",
    "hist_norm = historical_df['mid_price'] / historical_df['mid_price'].iloc[0] * 100\n",
    "synth_norm = synthetic_df['mid_price'] / synthetic_df['mid_price'].iloc[0] * 100\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(hist_norm))), y=hist_norm, name=\"Historical\", \n",
    "               line=dict(color=\"blue\", width=1)),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=list(range(len(synth_norm))), y=synth_norm, name=\"Synthetic\", \n",
    "               line=dict(color=\"orange\", width=1)),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Bar charts for key metrics\n",
    "metrics = ['returns_std_bps', 'returns_skewness', 'returns_kurtosis']\n",
    "hist_vals = [hist_stats[m] for m in metrics]\n",
    "synth_vals = [synth_stats[m] for m in metrics]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Std (bps)', 'Skewness', 'Kurtosis'], y=hist_vals, name=\"Historical\", marker_color=\"blue\"),\n",
    "    row=3, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Std (bps)', 'Skewness', 'Kurtosis'], y=synth_vals, name=\"Synthetic\", marker_color=\"orange\"),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "spread_metrics = ['spread_mean_bps', 'spread_std_bps', 'spread_median_bps']\n",
    "hist_spread_vals = [hist_stats[m] for m in spread_metrics]\n",
    "synth_spread_vals = [synth_stats[m] for m in spread_metrics]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Mean', 'Std', 'Median'], y=hist_spread_vals, name=\"Hist Spread\", marker_color=\"blue\"),\n",
    "    row=3, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Mean', 'Std', 'Median'], y=synth_spread_vals, name=\"Synth Spread\", marker_color=\"orange\"),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "depth_metrics = ['bid_depth_5_mean', 'ask_depth_5_mean']\n",
    "hist_depth_vals = [hist_stats[m] for m in depth_metrics]\n",
    "synth_depth_vals = [synth_stats[m] for m in depth_metrics]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Bid Depth', 'Ask Depth'], y=hist_depth_vals, name=\"Hist Depth\", marker_color=\"blue\"),\n",
    "    row=3, col=3\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['Bid Depth', 'Ask Depth'], y=synth_depth_vals, name=\"Synth Depth\", marker_color=\"orange\"),\n",
    "    row=3, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=900, title_text=\"MCP vs Historical Data: Summary Dashboard\", showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"MCP SYNTHETIC DATA vs HYPERLIQUID HISTORICAL DATA COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nData Source: Hyperliquid L2 Book - {SYMBOL}\")\n",
    "print(f\"Date: {DATA_DATE}\")\n",
    "print(f\"Records: {len(historical_df)} (resampled at {S3_CONFIG['resample_seconds']}s)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(f\"\\n1. VOLATILITY:\")\n",
    "print(f\"   Historical: {hist_stats['volatility_annual']:.2f}%\")\n",
    "print(f\"   Synthetic:  {synth_stats['volatility_annual']:.2f}%\")\n",
    "print(f\"   Match:      {vol_match_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n2. SPREAD DYNAMICS:\")\n",
    "print(f\"   Historical Mean: {hist_stats['spread_mean_bps']:.2f} bps\")\n",
    "print(f\"   Synthetic Mean:  {synth_stats['spread_mean_bps']:.2f} bps\")\n",
    "print(f\"   Match:           {spread_match_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. DISTRIBUTION SHAPE:\")\n",
    "print(f\"   Historical Kurtosis: {hist_stats['returns_kurtosis']:.2f}\")\n",
    "print(f\"   Synthetic Kurtosis:  {synth_stats['returns_kurtosis']:.2f}\")\n",
    "print(f\"   Both exhibit fat tails: {'Yes' if hist_stats['returns_kurtosis'] > 0 and synth_stats['returns_kurtosis'] > 0 else 'No'}\")\n",
    "\n",
    "print(f\"\\n4. VOLATILITY CLUSTERING:\")\n",
    "print(f\"   Historical ACF(1) sq returns: {hist_sq_returns_acf[1]:.4f}\")\n",
    "print(f\"   Synthetic ACF(1) sq returns:  {synth_sq_returns_acf[1]:.4f}\")\n",
    "print(f\"   Both show clustering: {'Yes' if hist_sq_returns_acf[1] > 0.05 and synth_sq_returns_acf[1] > 0.05 else 'Weak'}\")\n",
    "\n",
    "print(f\"\\n5. STATISTICAL TESTS:\")\n",
    "print(f\"   KS Test (Returns): p={ks_returns.pvalue:.4f} ({'Similar' if ks_returns.pvalue > 0.01 else 'Different'})\")\n",
    "print(f\"   KS Test (Spreads): p={ks_spreads.pvalue:.4f} ({'Similar' if ks_spreads.pvalue > 0.01 else 'Different'})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "overall_match = (vol_match_pct + spread_match_pct + kurt_match_pct) / 3\n",
    "print(f\"OVERALL SIMILARITY SCORE: {overall_match:.1f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if overall_match > 70:\n",
    "    print(\"\\nCONCLUSION: MCP synthetic data exhibits statistical properties\")\n",
    "    print(\"consistent with real Hyperliquid market data, making it suitable\")\n",
    "    print(\"for backtesting and strategy development.\")\n",
    "else:\n",
    "    print(\"\\nCONCLUSION: Some statistical differences exist. Consider tuning\")\n",
    "    print(\"MCP parameters to better match historical patterns.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison results\n",
    "output_dir = Path(\"./outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save comparison statistics\n",
    "comparison_stats.to_csv(output_dir / \"statistical_comparison.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    \"symbol\": SYMBOL,\n",
    "    \"date\": DATA_DATE,\n",
    "    \"historical_records\": len(historical_df),\n",
    "    \"synthetic_records\": len(synthetic_df),\n",
    "    \"historical_volatility\": hist_stats['volatility_annual'],\n",
    "    \"synthetic_volatility\": synth_stats['volatility_annual'],\n",
    "    \"historical_spread_bps\": hist_stats['spread_mean_bps'],\n",
    "    \"synthetic_spread_bps\": synth_stats['spread_mean_bps'],\n",
    "    \"ks_test_returns_pvalue\": ks_returns.pvalue,\n",
    "    \"ks_test_spreads_pvalue\": ks_spreads.pvalue,\n",
    "    \"overall_similarity_pct\": overall_match,\n",
    "}\n",
    "\n",
    "with open(output_dir / \"comparison_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Results exported to {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.12.5",
   "language": "python",
   "name": "py3.12.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}